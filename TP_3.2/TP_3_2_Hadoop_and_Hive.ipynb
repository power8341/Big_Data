{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwjvirExIQAM"
      },
      "source": [
        "#Hive with Hadoop\n",
        "This notebook has all the codes / commands required to install Hadoop and Hive <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myPIGP-mwKBD"
      },
      "source": [
        "#1 Hadoop\n",
        "Hadoop is a pre-requisite for Hive <br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9bT9M1yvyXG"
      },
      "source": [
        "## 1.1 Download, Install Hadoop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXFZuorwF25e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a5cbefe-74a3-4704-9016-504fd2f53cd7"
      },
      "source": [
        "# The default JVM available at /usr/lib/jvm/java-11-openjdk-amd64/  works for Hadoop\n",
        "# But gives errors with Hive https://stackoverflow.com/questions/54037773/hive-exception-class-jdk-internal-loader-classloadersappclassloader-cannot\n",
        "# Hence this JVM needs to be installed\n",
        "!apt-get update > /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "\n",
        "# If there is an error in this cell, it is very likely that the version of hadoop has changed\n",
        "# Download the latest version of Hadoop and change the version numbers accordingly\n",
        "#wget -q https://downloads.apache.org/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz\n",
        "#!wget -q https://downloads.apache.org/hadoop/common/hadoop-3.3.2/hadoop-3.3.2.tar.gz\n",
        "#!wget  https://downloads.apache.org/hadoop/common/hadoop-3.3.2/hadoop-3.3.2.tar.gz\n",
        "!wget -q https://downloads.apache.org/hadoop/common/hadoop-3.3.5/hadoop-3.3.5.tar.gz\n",
        "# Unzip it\n",
        "# the tar command with the -x flag to extract, -z to uncompress, -v for verbose output, and -f to specify that we’re extracting from a file\n",
        "#!tar -xzf hadoop-3.3.2.tar.gz\n",
        "!tar -xzf hadoop-3.3.5.tar.gz\n",
        "#copy  hadoop file to user/local\n",
        "#!mv  hadoop-3.3.2/ /usr/local/\n",
        "!mv  hadoop-3.3.5/ /usr/local/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh6Dqbbrwqpe"
      },
      "source": [
        "## 1.2 Set Environment Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ez4T7Gs3RAn"
      },
      "source": [
        "#To set java path, go to /usr/local/hadoop-3.3.0/etc/hadoop/hadoop-env.sh then\n",
        "#. . . export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/ . . .\n",
        "#we have used a simpler alternative route using os.environ - it works\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"   # default is changed\n",
        "#os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64/\"\n",
        "# make sure that the version number is as downloaded\n",
        "#os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.3.0/\"\n",
        "#os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.3.2/\"\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.3.5/\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDFgpWGLhdhl",
        "outputId": "c7bd1c48-c837-473d-95e6-137f771dea56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Add Hadoop BIN to PATH\n",
        "# Get the current_path from output of previous command\n",
        "#current_path = '/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin'\n",
        "#current_path = '/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin'\n",
        "#new_path = current_path+':/usr/local/hadoop-3.3.2/bin/'\n",
        "#os.environ[\"PATH\"] = new_path\n",
        "#!echo $PATH\n",
        "\n",
        "current_path = os.getenv('PATH')\n",
        "#new_path = current_path+':/usr/local/hadoop-3.3.0/bin/'\n",
        "#new_path = current_path+':/usr/local/hadoop-3.3.2/bin/'\n",
        "new_path = current_path+':/usr/local/hadoop-3.3.5/bin/'\n",
        "os.environ[\"PATH\"] = new_path\n",
        "!echo $PATH"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/usr/local/hadoop-3.3.5/bin/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj00rPPZyEWZ"
      },
      "source": [
        "## 1.3 Test Hadoop Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zhf-zK7NcBDF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2ea1565-c7aa-4625-de3b-b04dede150a5"
      },
      "source": [
        "#Running Hadoop - Test RUN, not doing anything at all\n",
        "!/usr/local/hadoop-3.3.5/bin/hadoop\n",
        "# UNCOMMENT the following line if you want to make sure that Hadoop is alive!\n",
        "!hadoop"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
            " or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]\n",
            "  where CLASSNAME is a user-provided Java class\n",
            "\n",
            "  OPTIONS is none or any of:\n",
            "\n",
            "buildpaths                       attempt to add class files from build tree\n",
            "--config dir                     Hadoop config directory\n",
            "--debug                          turn on shell script debug mode\n",
            "--help                           usage information\n",
            "hostnames list[,of,host,names]   hosts to use in worker mode\n",
            "hosts filename                   list of hosts to use in worker mode\n",
            "loglevel level                   set the log4j level for this command\n",
            "workers                          turn on worker mode\n",
            "\n",
            "  SUBCOMMAND is one of:\n",
            "\n",
            "\n",
            "    Admin Commands:\n",
            "\n",
            "daemonlog     get/set the log level for each daemon\n",
            "\n",
            "    Client Commands:\n",
            "\n",
            "archive       create a Hadoop archive\n",
            "checknative   check native Hadoop and compression libraries availability\n",
            "classpath     prints the class path needed to get the Hadoop jar and the required libraries\n",
            "conftest      validate configuration XML files\n",
            "credential    interact with credential providers\n",
            "distch        distributed metadata changer\n",
            "distcp        copy file or directories recursively\n",
            "dtutil        operations related to delegation tokens\n",
            "envvars       display computed Hadoop environment variables\n",
            "fs            run a generic filesystem user client\n",
            "gridmix       submit a mix of synthetic job, modeling a profiled from production load\n",
            "jar <jar>     run a jar file. NOTE: please use \"yarn jar\" to launch YARN applications, not this\n",
            "              command.\n",
            "jnipath       prints the java.library.path\n",
            "kdiag         Diagnose Kerberos Problems\n",
            "kerbname      show auth_to_local principal conversion\n",
            "key           manage keys via the KeyProvider\n",
            "rumenfolder   scale a rumen input trace\n",
            "rumentrace    convert logs into a rumen trace\n",
            "s3guard       S3 Commands\n",
            "trace         view and modify Hadoop tracing settings\n",
            "version       print the version\n",
            "\n",
            "    Daemon Commands:\n",
            "\n",
            "kms           run KMS, the Key Management Server\n",
            "registrydns   run the registry DNS server\n",
            "\n",
            "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n",
            "Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
            " or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]\n",
            "  where CLASSNAME is a user-provided Java class\n",
            "\n",
            "  OPTIONS is none or any of:\n",
            "\n",
            "buildpaths                       attempt to add class files from build tree\n",
            "--config dir                     Hadoop config directory\n",
            "--debug                          turn on shell script debug mode\n",
            "--help                           usage information\n",
            "hostnames list[,of,host,names]   hosts to use in worker mode\n",
            "hosts filename                   list of hosts to use in worker mode\n",
            "loglevel level                   set the log4j level for this command\n",
            "workers                          turn on worker mode\n",
            "\n",
            "  SUBCOMMAND is one of:\n",
            "\n",
            "\n",
            "    Admin Commands:\n",
            "\n",
            "daemonlog     get/set the log level for each daemon\n",
            "\n",
            "    Client Commands:\n",
            "\n",
            "archive       create a Hadoop archive\n",
            "checknative   check native Hadoop and compression libraries availability\n",
            "classpath     prints the class path needed to get the Hadoop jar and the required libraries\n",
            "conftest      validate configuration XML files\n",
            "credential    interact with credential providers\n",
            "distch        distributed metadata changer\n",
            "distcp        copy file or directories recursively\n",
            "dtutil        operations related to delegation tokens\n",
            "envvars       display computed Hadoop environment variables\n",
            "fs            run a generic filesystem user client\n",
            "gridmix       submit a mix of synthetic job, modeling a profiled from production load\n",
            "jar <jar>     run a jar file. NOTE: please use \"yarn jar\" to launch YARN applications, not this\n",
            "              command.\n",
            "jnipath       prints the java.library.path\n",
            "kdiag         Diagnose Kerberos Problems\n",
            "kerbname      show auth_to_local principal conversion\n",
            "key           manage keys via the KeyProvider\n",
            "rumenfolder   scale a rumen input trace\n",
            "rumentrace    convert logs into a rumen trace\n",
            "s3guard       S3 Commands\n",
            "trace         view and modify Hadoop tracing settings\n",
            "version       print the version\n",
            "\n",
            "    Daemon Commands:\n",
            "\n",
            "kms           run KMS, the Key Management Server\n",
            "registrydns   run the registry DNS server\n",
            "\n",
            "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUlA5c3yRCx1"
      },
      "source": [
        "#2 Hive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pURJ-sKVsymi"
      },
      "source": [
        "## 2.1 Download, Install HIVE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFsywGzPRaYp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbc824aa-16a9-455f-8dbf-60ee64ddca5f"
      },
      "source": [
        "# Download and Unzip the correct version and unzip\n",
        "!wget https://archive.apache.org/dist/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gz\n",
        "#!wget -q https://downloads.apache.org/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz\n",
        "\n",
        "!tar xzf apache-hive-3.1.2-bin.tar.gz\n",
        "#!tar xzf apache-hive-3.1.3-bin.tar.gz"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-26 11:07:33--  https://archive.apache.org/dist/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 278813748 (266M) [application/x-gzip]\n",
            "Saving to: ‘apache-hive-3.1.2-bin.tar.gz’\n",
            "\n",
            "apache-hive-3.1.2-b 100%[===================>] 265.90M  10.5MB/s    in 63s     \n",
            "\n",
            "2025-03-26 11:08:36 (4.25 MB/s) - ‘apache-hive-3.1.2-bin.tar.gz’ saved [278813748/278813748]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq6QYCVetNED"
      },
      "source": [
        "## 2.2 Set Environment *Variables*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qROUu4vSdEU",
        "outputId": "661c5991-9b7d-4948-834b-dce7dfb441fb"
      },
      "source": [
        "# Make sure that the version number is correct and is as downloaded\n",
        "os.environ[\"HIVE_HOME\"] = \"/content/apache-hive-3.1.2-bin\"\n",
        "#os.environ[\"HIVE_HOME\"] = \"/content/apache-hive-3.1.3-bin\"\n",
        "!echo $HIVE_HOME"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/apache-hive-3.1.2-bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx3pKQ9PTBfR",
        "outputId": "eda13fcc-0618-444a-8dfe-8753f7be6271"
      },
      "source": [
        "# current_path is set from output of previous command\n",
        "#current_path = '/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin:/usr/local/hadoop-3.3.0/bin/'\n",
        "#current_path = '/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/usr/local/hadoop-3.3.2/bin/'\n",
        "#new_path = current_path+':/content/apache-hive-3.1.2-bin/bin'\n",
        "#os.environ[\"PATH\"] = new_path\n",
        "!echo $PATH\n",
        "\n",
        "\n",
        "current_path = os.getenv('PATH')\n",
        "#new_path = current_path+':/usr/local/hadoop-3.3.0/bin/'\n",
        "#new_path = current_path+':/content/apache-hive-3.1.2-bin/bin'\n",
        "new_path = current_path+':/content/apache-hive-3.1.2-bin/bin'\n",
        "os.environ[\"PATH\"] = new_path\n",
        "!echo $PATH"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/usr/local/hadoop-3.3.5/bin/\n",
            "/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/usr/local/hadoop-3.3.5/bin/:/content/apache-hive-3.1.2-bin/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfiA2LItT_L2",
        "outputId": "39dccfed-7ede-4961-afe9-ec71c181d23c"
      },
      "source": [
        "!echo $JAVA_HOME\n",
        "!echo $HADOOP_HOME\n",
        "!echo $HIVE_HOME"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-8-openjdk-amd64\n",
            "/usr/local/hadoop-3.3.5/\n",
            "/content/apache-hive-3.1.2-bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AryjHG4ltfEe"
      },
      "source": [
        "## 2.3 Set up HDFS Directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dry58UPMVTat",
        "outputId": "eb4c3476-49e0-4231-e877-9ae5a93baaba"
      },
      "source": [
        "!hdfs dfs -mkdir /tmp\n",
        "!hdfs dfs -chmod g+w /tmp\n",
        "#!hdfs dfs -ls /\n",
        "!hdfs dfs -mkdir -p /content/warehouse\n",
        "!hdfs dfs -chmod g+w /content/warehouse\n",
        "#!hdfs dfs -ls /content/"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: `/tmp': File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VrvjhfG2JXs"
      },
      "source": [
        "## 2.4 Initialise HIVE - note and fix errors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLX7AvL8YLMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "681ecc77-02c0-4f34-8cfa-f0c2e64adda3"
      },
      "source": [
        "# TYPE this command, do not copy and paste. Non printing characters cause havoc\n",
        "# There will be two errors, that we will fix\n",
        "# UNCOMMENT the following line if you WISH TO SEE the errors\n",
        "!schematool -initSchema -dbType derby\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SLF4J: Class path contains multiple SLF4J bindings.\n",
            "SLF4J: Found binding in [jar:file:/content/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.3.5/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
            "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
            "Metastore connection URL:\t jdbc:derby:;databaseName=metastore_db;create=true\n",
            "Metastore Connection Driver :\t org.apache.derby.jdbc.EmbeddedDriver\n",
            "Metastore connection User:\t APP\n",
            "Starting metastore schema initialization to 3.1.0\n",
            "Initialization script hive-schema-3.1.0.derby.sql\n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            " \n",
            " \n",
            "\n",
            " \n",
            " \n",
            " \n",
            "\n",
            "\n",
            " \n",
            " \n",
            " \n",
            "\n",
            " \n",
            " \n",
            " \n",
            "\n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            "\n",
            " \n",
            " \n",
            " \n",
            "\n",
            " \n",
            " \n",
            " \n",
            "\n",
            " \n",
            " \n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            " \n",
            " \n",
            " \n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            " \n",
            "\n",
            "\n",
            "Initialization script completed\n",
            "schemaTool completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v21CxgGLuJPQ"
      },
      "source": [
        "### 2.4.1 Fix One Warning, One Error\n",
        "SLF4J is duplicate, need to locate them and remove one <br>\n",
        "Guava jar version is low"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Becy3BABuE8b",
        "outputId": "891641d5-fecc-414b-c9e8-67064a33319c"
      },
      "source": [
        "# locate multiple instances of slf4j ...\n",
        "!ls $HADOOP_HOME/share/hadoop/common/lib/*slf4j*\n",
        "!ls $HIVE_HOME/lib/*slf4j*"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/hadoop-3.3.5//share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar\n",
            "/usr/local/hadoop-3.3.5//share/hadoop/common/lib/slf4j-api-1.7.36.jar\n",
            "/usr/local/hadoop-3.3.5//share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar\n",
            "/content/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEUomnHGu4kR"
      },
      "source": [
        "# removed the logging jar from Hive, retaining the Hadoop jar\n",
        "!mv /content/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar ./\n",
        "#!mv /content/apache-hive-3.1.3-bin/lib/log4j-slf4j-impl-2.17.1.jar ./"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwLAYh7TY4ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4332ca7-6b11-47ff-f52b-3145a8548b86"
      },
      "source": [
        "# guava jar needs to above v 20\n",
        "# https://stackoverflow.com/questions/45247193/nosuchmethoderror-com-google-common-base-preconditions-checkargumentzljava-lan\n",
        "!ls $HIVE_HOME/lib/gu*"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/apache-hive-3.1.2-bin/lib/guava-19.0.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHpmyrbkZFad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04505534-b8a2-4490-dc70-9bddc80cd35e"
      },
      "source": [
        "# the one available with Hadoop is better, v 27\n",
        "!ls $HADOOP_HOME/share/hadoop/hdfs/lib/gu*"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/hadoop-3.3.5//share/hadoop/hdfs/lib/guava-27.0-jre.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ3Ex1vyZeZG"
      },
      "source": [
        "# Remove the Hive Guava and replace with Hadoop Guava\n",
        "!mv $HIVE_HOME/lib/guava-19.0.jar ./\n",
        "\n",
        "!cp $HADOOP_HOME/share/hadoop/hdfs/lib/guava-27.0-jre.jar $HIVE_HOME/lib/"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdtNOXy8v4iD"
      },
      "source": [
        "##2.5 Initialize HIVE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install colab-xterm\n",
        "%load_ext colabxterm"
      ],
      "metadata": {
        "id": "S4ITBCrOGVCU",
        "outputId": "9b95a61a-7a6f-4959-82ef-2752605003c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/115.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m112.6/115.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.6/115.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%xterm"
      ],
      "metadata": {
        "id": "KWoYYPjgGbBN"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Tzw4XkApRg3"
      },
      "source": [
        "#Type this command, dont copy-paste\n",
        "# Non printing characters inside the command will give totally illogical errors\n",
        "#!schematool -initSchema -dbType derby"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nALF720ewT_-"
      },
      "source": [
        "## 2.6 Test HIVE\n",
        "1. Create database\n",
        "2. Create table\n",
        "3. Insert data\n",
        "4. Retrieve data\n",
        "\n",
        "using command line options as [given here](https://cwiki.apache.org/confluence/display/hive/languagemanual+cli#)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKBG__HrKt9N",
        "outputId": "3b5d6551-8f90-4018-e1b6-11fe598d8bb2"
      },
      "source": [
        "!hive -e \"\\\n",
        "create database if not exists bikramDB;\\\n",
        "show databases;\\\n",
        "\""
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hive Session ID = 4c45461a-5ee1-4cdb-9401-a1bc74c94db7\n",
            "\n",
            "Logging initialized using configuration in jar:file:/content/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true\n",
            "Hive Session ID = 30fe4a36-3c3b-4e03-b9a7-019b34fa465a\n",
            "OK\n",
            "Time taken: 1.426 seconds\n",
            "OK\n",
            "bikramdb\n",
            "default\n",
            "Time taken: 0.292 seconds, Fetched: 2 row(s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duhKRlsnax6U",
        "outputId": "066afd46-a7df-4eb4-eb79-9152c1490a3d"
      },
      "source": [
        "!hive -database bikramDB -e \"\\\n",
        "create table if not exists emp2 (name string, age int);\\\n",
        "show tables;\\\n",
        "\""
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hive Session ID = e6d7a353-4638-45ef-a1f5-b9b6c44ff68d\n",
            "\n",
            "Logging initialized using configuration in jar:file:/content/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true\n",
            "Hive Session ID = b64701b9-00b8-4ce5-83dd-b4cf44380d3d\n",
            "OK\n",
            "Time taken: 1.602 seconds\n",
            "OK\n",
            "Time taken: 1.538 seconds\n",
            "OK\n",
            "emp2\n",
            "Time taken: 0.249 seconds, Fetched: 1 row(s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlXhPGIBbd7v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "126aa5c4-2ab0-4056-da69-6946e2568832"
      },
      "source": [
        "!hive -database bikramDB -e \"show tables\""
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hive Session ID = 2de0dbe7-2794-442c-82a3-8000ea7020f0\n",
            "\n",
            "Logging initialized using configuration in jar:file:/content/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true\n",
            "Hive Session ID = e241466a-4482-449e-be70-9ef4d3c7d716\n",
            "OK\n",
            "Time taken: 0.978 seconds\n",
            "OK\n",
            "emp2\n",
            "Time taken: 0.597 seconds, Fetched: 1 row(s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBbc2fzSb4Po",
        "outputId": "1e729429-417f-452f-ad1b-32500150279a"
      },
      "source": [
        "!hive -database bikramDB -e \"\\\n",
        "insert into emp2 values ('Maxim', 70);\\\n",
        "insert into emp2 values ('Ade', 49);\\\n",
        "\""
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hive Session ID = 66354075-fe17-451a-afe6-652154326cff\n",
            "\n",
            "Logging initialized using configuration in jar:file:/content/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true\n",
            "Hive Session ID = bbba8aae-ddb6-4ab9-a73c-6f07458585f1\n",
            "OK\n",
            "Time taken: 0.953 seconds\n",
            "Query ID = root_20250326111537_b62ff887-1c02-49aa-ae0e-d3b4a31b9e1d\n",
            "Total jobs = 3\n",
            "Launching Job 1 out of 3\n",
            "Number of reduce tasks determined at compile time: 1\n",
            "In order to change the average load for a reducer (in bytes):\n",
            "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
            "In order to limit the maximum number of reducers:\n",
            "  set hive.exec.reducers.max=<number>\n",
            "In order to set a constant number of reducers:\n",
            "  set mapreduce.job.reduces=<number>\n",
            "Job running in-process (local Hadoop)\n",
            "2025-03-26 11:15:44,695 Stage-1 map = 0%,  reduce = 0%\n",
            "2025-03-26 11:15:45,705 Stage-1 map = 100%,  reduce = 100%\n",
            "Ended Job = job_local802644767_0001\n",
            "Stage-4 is selected by condition resolver.\n",
            "Stage-3 is filtered out by condition resolver.\n",
            "Stage-5 is filtered out by condition resolver.\n",
            "Moving data to directory file:/user/hive/warehouse/bikramdb.db/emp2/.hive-staging_hive_2025-03-26_11-15-37_890_2072046136086058304-1/-ext-10000\n",
            "Loading data to table bikramdb.emp2\n",
            "MapReduce Jobs Launched: \n",
            "Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n",
            "Total MapReduce CPU Time Spent: 0 msec\n",
            "OK\n",
            "Time taken: 9.418 seconds\n",
            "Query ID = root_20250326111547_df7abdda-602b-4aaf-9531-6827345fe0ea\n",
            "Total jobs = 3\n",
            "Launching Job 1 out of 3\n",
            "Number of reduce tasks determined at compile time: 1\n",
            "In order to change the average load for a reducer (in bytes):\n",
            "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
            "In order to limit the maximum number of reducers:\n",
            "  set hive.exec.reducers.max=<number>\n",
            "In order to set a constant number of reducers:\n",
            "  set mapreduce.job.reduces=<number>\n",
            "Job running in-process (local Hadoop)\n",
            "2025-03-26 11:15:49,207 Stage-1 map = 100%,  reduce = 100%\n",
            "Ended Job = job_local980437040_0002\n",
            "Stage-4 is selected by condition resolver.\n",
            "Stage-3 is filtered out by condition resolver.\n",
            "Stage-5 is filtered out by condition resolver.\n",
            "Moving data to directory file:/user/hive/warehouse/bikramdb.db/emp2/.hive-staging_hive_2025-03-26_11-15-47_310_5285668800513182171-1/-ext-10000\n",
            "Loading data to table bikramdb.emp2\n",
            "MapReduce Jobs Launched: \n",
            "Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n",
            "Total MapReduce CPU Time Spent: 0 msec\n",
            "OK\n",
            "Time taken: 2.278 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFxnPRbQclhP",
        "outputId": "4634daa6-91aa-40d7-8f02-4c5600c3ffe5"
      },
      "source": [
        "!hive -database bikramDB -e \"\\\n",
        "select * from emp2;\\\n",
        "select * from emp2 where name = 'Ade';\\\n",
        "\""
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hive Session ID = 3dc50016-2e37-4703-9400-c6bb3c26bc57\n",
            "\n",
            "Logging initialized using configuration in jar:file:/content/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true\n",
            "Hive Session ID = bf022dbb-59f4-48d9-a1a2-4065c03bc220\n",
            "OK\n",
            "Time taken: 0.989 seconds\n",
            "OK\n",
            "Ade\t49\n",
            "Maxim\t70\n",
            "Time taken: 3.03 seconds, Fetched: 2 row(s)\n",
            "OK\n",
            "Ade\t49\n",
            "Time taken: 0.693 seconds, Fetched: 1 row(s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.0 Bulk Data - Super Store Data"
      ],
      "metadata": {
        "id": "SHUcIMmrbo4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Data File from this link : https://drive.google.com/file/d/1UEFd6IMjug_7jKLHu9IOHIIQZ0LhbfsL/view?usp=drive_link .You can put it here /content/SS_Orders.csv"
      ],
      "metadata": {
        "id": "VRVjbJFXmc2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get Data File from this link : https://drive.google.com/file/d/1UEFd6IMjug_7jKLHu9IOHIIQZ0LhbfsL/view?usp=drive_link .You can put it here /content/SS_Orders.csv"
      ],
      "metadata": {
        "id": "dXHItaHIxQjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hive -S -database bikramDB -e \"\\\n",
        "drop table if exists ss_order;\\\n",
        "CREATE TABLE IF NOT EXISTS ss_order (\\\n",
        "    RowID smallint,\\\n",
        "    OrderID char(14),OrderDate string,\\\n",
        "    ShipDate string,ShipMode varchar(16),\\\n",
        "    CustomerID char(8),CustomerName varchar(30),Segment varchar(20),\\\n",
        "    Country varchar(30),City varchar(30),State varchar(30),PostalCode char(5),Region varchar(15) ,\\\n",
        "    ProductID varchar(20), Category varchar(40), SubCategory varchar(40), ProductName varchar(200), \\\n",
        "    Sales decimal(8,2), Quantity smallint, Discount decimal(4,2), Profit decimal(8,2) \\\n",
        ") row format delimited fields terminated by ','; \\\n",
        "describe ss_order;\\\n",
        "\""
      ],
      "metadata": {
        "id": "pRebpZNKcJUO",
        "outputId": "86b31ee4-81ed-426d-fe90-abe910c52979",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hive Session ID = 2d7c6f19-d773-4199-90b0-18c7eab89da9\n",
            "Hive Session ID = 8599262d-4e47-4541-8db0-11dd03f337dd\n",
            "rowid               \tsmallint            \t                    \n",
            "orderid             \tchar(14)            \t                    \n",
            "orderdate           \tstring              \t                    \n",
            "shipdate            \tstring              \t                    \n",
            "shipmode            \tvarchar(16)         \t                    \n",
            "customerid          \tchar(8)             \t                    \n",
            "customername        \tvarchar(30)         \t                    \n",
            "segment             \tvarchar(20)         \t                    \n",
            "country             \tvarchar(30)         \t                    \n",
            "city                \tvarchar(30)         \t                    \n",
            "state               \tvarchar(30)         \t                    \n",
            "postalcode          \tchar(5)             \t                    \n",
            "region              \tvarchar(15)         \t                    \n",
            "productid           \tvarchar(20)         \t                    \n",
            "category            \tvarchar(40)         \t                    \n",
            "subcategory         \tvarchar(40)         \t                    \n",
            "productname         \tvarchar(200)        \t                    \n",
            "sales               \tdecimal(8,2)        \t                    \n",
            "quantity            \tsmallint            \t                    \n",
            "discount            \tdecimal(4,2)        \t                    \n",
            "profit              \tdecimal(8,2)        \t                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remove the CRLF character from the end of the row if it exists\n",
        "#!sed 's/[^a-zA-Z0-9,\\/\\.\\-]/ /g' SS_Orders.csv > datafile.csv\n",
        "!sed 's/\\r//' /content/SS_Orders.csv > datafile.csv\n",
        "#!sed 's/\\r//' /content/eCommerce_02PC_2021.csv > datafile.csv\n",
        "# remove the first line containing headers from the file\n",
        "!sed -i -e \"1d\" datafile.csv\n",
        "!head datafile.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TS5uXZ8jnL93",
        "outputId": "55134c2a-4182-4bbb-f2c5-5342d9213a4e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1,CA-2016-152156,08/11/2016,11/11/2016,Second Class,CG-12520,Claire Gute,Consumer,United States,Henderson,Kentucky,42420,South,FUR-BO-10001798,Furniture,Bookcases,Bush Somerset Collection Bookcase,261.96,2,0,41.9136\n",
            "2,CA-2016-152156,08/11/2016,11/11/2016,Second Class,CG-12520,Claire Gute,Consumer,United States,Henderson,Kentucky,42420,South,FUR-CH-10000454,Furniture,Chairs,\"Hon Deluxe Fabric Upholstered Stacking Chairs, Rounded Back\",731.94,3,0,219.582\n",
            "3,CA-2016-138688,12/06/2016,16/06/2016,Second Class,DV-13045,Darrin Van Huff,Corporate,United States,Los Angeles,California,90036,West,OFF-LA-10000240,Office Supplies,Labels,Self-Adhesive Address Labels for Typewriters by Universal,14.62,2,0,6.8714\n",
            "4,US-2015-108966,11/10/2015,18/10/2015,Standard Class,SO-20335,Sean O'Donnell,Consumer,United States,Fort Lauderdale,Florida,33311,South,FUR-TA-10000577,Furniture,Tables,Bretford CR4500 Series Slim Rectangular Table,957.5775,5,0.45,-383.031\n",
            "5,US-2015-108966,11/10/2015,18/10/2015,Standard Class,SO-20335,Sean O'Donnell,Consumer,United States,Fort Lauderdale,Florida,33311,South,OFF-ST-10000760,Office Supplies,Storage,Eldon Fold 'N Roll Cart System,22.368,2,0.2,2.5164\n",
            "6,CA-2014-115812,09/06/2014,14/06/2014,Standard Class,BH-11710,Brosina Hoffman,Consumer,United States,Los Angeles,California,90032,West,FUR-FU-10001487,Furniture,Furnishings,\"Eldon Expressions Wood and Plastic Desk Accessories, Cherry Wood\",48.86,7,0,14.1694\n",
            "7,CA-2014-115812,09/06/2014,14/06/2014,Standard Class,BH-11710,Brosina Hoffman,Consumer,United States,Los Angeles,California,90032,West,OFF-AR-10002833,Office Supplies,Art,Newell 322,7.28,4,0,1.9656\n",
            "8,CA-2014-115812,09/06/2014,14/06/2014,Standard Class,BH-11710,Brosina Hoffman,Consumer,United States,Los Angeles,California,90032,West,TEC-PH-10002275,Technology,Phones,Mitel 5320 IP Phone VoIP phone,907.152,6,0.2,90.7152\n",
            "9,CA-2014-115812,09/06/2014,14/06/2014,Standard Class,BH-11710,Brosina Hoffman,Consumer,United States,Los Angeles,California,90032,West,OFF-BI-10003910,Office Supplies,Binders,DXL Angle-View Binders with Locking Rings by Samsill,18.504,3,0.2,5.7825\n",
            "10,CA-2014-115812,09/06/2014,14/06/2014,Standard Class,BH-11710,Brosina Hoffman,Consumer,United States,Los Angeles,California,90032,West,OFF-AP-10002892,Office Supplies,Appliances,Belkin F5C206VTEL 6 Outlet Surge,114.9,5,0,34.47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hive -S -database bikramDB -e \"\\\n",
        "TRUNCATE TABLE ss_order;\\\n",
        "LOAD DATA LOCAL INPATH 'datafile.csv' INTO TABLE ss_order;\\\n",
        "\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDPy6d7Em3EX",
        "outputId": "4e8f3ddd-14ca-4e92-b59e-5346da66f798"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hive Session ID = 737dcbcf-e2cc-4fb4-9cf2-8869a13942aa\n",
            "Hive Session ID = 58b574b5-78b8-4ecd-979b-ee808aff19e2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hive -S -database bikramDB -e \"\\\n",
        "select count(*) from ss_order;\\\n",
        "select * from ss_order limit 10;\\\n",
        "\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hyx6Z4VUsZ6",
        "outputId": "3adc5399-ce2e-4d89-f8a4-b3295a03841e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hive Session ID = 63063621-3733-4038-8b7c-0ee2ead1c4b8\n",
            "Hive Session ID = 8fb9e34f-2a14-4b13-9135-31abfa061e41\n",
            "9994\n",
            "1\tCA-2016-152156\t08/11/2016\t11/11/2016\tSecond Class\tCG-12520\tClaire Gute\tConsumer\tUnited States\tHenderson\tKentucky\t42420\tSouth\tFUR-BO-10001798\tFurniture\tBookcases\tBush Somerset Collection Bookcase\t261.96\t2\t0.00\t41.91\n",
            "2\tCA-2016-152156\t08/11/2016\t11/11/2016\tSecond Class\tCG-12520\tClaire Gute\tConsumer\tUnited States\tHenderson\tKentucky\t42420\tSouth\tFUR-CH-10000454\tFurniture\tChairs\t\"Hon Deluxe Fabric Upholstered Stacking Chairs\tNULL\t731\t3.00\t0.00\n",
            "3\tCA-2016-138688\t12/06/2016\t16/06/2016\tSecond Class\tDV-13045\tDarrin Van Huff\tCorporate\tUnited States\tLos Angeles\tCalifornia\t90036\tWest\tOFF-LA-10000240\tOffice Supplies\tLabels\tSelf-Adhesive Address Labels for Typewriters by Universal\t14.62\t2\t0.00\t6.87\n",
            "4\tUS-2015-108966\t11/10/2015\t18/10/2015\tStandard Class\tSO-20335\tSean O'Donnell\tConsumer\tUnited States\tFort Lauderdale\tFlorida\t33311\tSouth\tFUR-TA-10000577\tFurniture\tTables\tBretford CR4500 Series Slim Rectangular Table\t957.58\t5\t0.45\t-383.03\n",
            "5\tUS-2015-108966\t11/10/2015\t18/10/2015\tStandard Class\tSO-20335\tSean O'Donnell\tConsumer\tUnited States\tFort Lauderdale\tFlorida\t33311\tSouth\tOFF-ST-10000760\tOffice Supplies\tStorage\tEldon Fold 'N Roll Cart System\t22.37\t2\t0.20\t2.52\n",
            "6\tCA-2014-115812\t09/06/2014\t14/06/2014\tStandard Class\tBH-11710\tBrosina Hoffman\tConsumer\tUnited States\tLos Angeles\tCalifornia\t90032\tWest\tFUR-FU-10001487\tFurniture\tFurnishings\t\"Eldon Expressions Wood and Plastic Desk Accessories\tNULL\t48\t7.00\t0.00\n",
            "7\tCA-2014-115812\t09/06/2014\t14/06/2014\tStandard Class\tBH-11710\tBrosina Hoffman\tConsumer\tUnited States\tLos Angeles\tCalifornia\t90032\tWest\tOFF-AR-10002833\tOffice Supplies\tArt\tNewell 322\t7.28\t4\t0.00\t1.97\n",
            "8\tCA-2014-115812\t09/06/2014\t14/06/2014\tStandard Class\tBH-11710\tBrosina Hoffman\tConsumer\tUnited States\tLos Angeles\tCalifornia\t90032\tWest\tTEC-PH-10002275\tTechnology\tPhones\tMitel 5320 IP Phone VoIP phone\t907.15\t6\t0.20\t90.72\n",
            "9\tCA-2014-115812\t09/06/2014\t14/06/2014\tStandard Class\tBH-11710\tBrosina Hoffman\tConsumer\tUnited States\tLos Angeles\tCalifornia\t90032\tWest\tOFF-BI-10003910\tOffice Supplies\tBinders\tDXL Angle-View Binders with Locking Rings by Samsill\t18.50\t3\t0.20\t5.78\n",
            "10\tCA-2014-115812\t09/06/2014\t14/06/2014\tStandard Class\tBH-11710\tBrosina Hoffman\tConsumer\tUnited States\tLos Angeles\tCalifornia\t90032\tWest\tOFF-AP-10002892\tOffice Supplies\tAppliances\tBelkin F5C206VTEL 6 Outlet Surge\t114.90\t5\t0.00\t34.47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hive -S -database bikramDB -e \"\\\n",
        "SELECT segment, SUM (sales) sales \\\n",
        "FROM ss_order \\\n",
        "GROUP BY segment; \\\n",
        "\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKgNHxIMoKO5",
        "outputId": "a1b2ccf6-4e47-4812-eb64-65b32bf63909"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hive Session ID = 26c41b71-de35-4c12-b2f7-63fbc1d70e76\n",
            "Hive Session ID = 6136ca16-6c15-4911-85ea-d9e7097b8f88\n",
            "Consumer\t979461.88\n",
            "Corporate\t568730.72\n",
            "Home Office\t365337.04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introducing SERDE"
      ],
      "metadata": {
        "id": "H4Ir_EWRvAue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hive -S -database bikramDB -e \"\\\n",
        "drop table if exists ss_order2;\\\n",
        "CREATE TABLE IF NOT EXISTS ss_order2 (\\\n",
        "    RowID smallint,\\\n",
        "    OrderID char(14),OrderDate string,\\\n",
        "    ShipDate string,ShipMode varchar(16),\\\n",
        "    CustomerID char(8),CustomerName varchar(30),Segment varchar(20),\\\n",
        "    Country varchar(30),City varchar(30),State varchar(30),PostalCode char(5),Region varchar(15) ,\\\n",
        "    ProductID varchar(20), Category varchar(40), SubCategory varchar(40), ProductName varchar(200), \\\n",
        "    Sales decimal(8,2), Quantity smallint, Discount decimal(4,2), Profit decimal(8,2) \\\n",
        ") ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' STORED AS TEXTFILE; \\\n",
        "\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mNsTGXdoZ77",
        "outputId": "f00134a4-3755-4244-ae2f-105b92ee595b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hive Session ID = a8783ca4-9085-4bb8-b5ca-646c1da78bbc\n",
            "Hive Session ID = 321bba82-ea3d-46a3-b34a-acda13d31d05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hive -S -database bikramDB -e \"\\\n",
        "TRUNCATE TABLE ss_order2;\\\n",
        "LOAD DATA LOCAL INPATH 'datafile.csv' INTO TABLE ss_order2;\\\n",
        "\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdBmfKZ7pKRc",
        "outputId": "dc9c19de-7350-47f9-dabd-2a89748b1a5c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hive Session ID = 9161f95a-d67a-43b7-8108-1e73d7d0b15d\n",
            "Hive Session ID = 97774bc1-d129-4511-89fb-f3d33e86808e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hive -S -database bikramDB -e \"\\\n",
        "SELECT segment, SUM (sales) sales \\\n",
        "FROM ss_order2 \\\n",
        "GROUP BY segment; \\\n",
        "\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlVpoOyeo5CA",
        "outputId": "d1162785-d1ee-4e0e-ada0-dd120d4d3b31"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hive Session ID = 1e9f9f9e-b75e-4400-954c-cafb02957d48\n",
            "Hive Session ID = 0fb7d301-ee9f-475c-81d7-071db44dde8c\n",
            "Consumer\t1161401.3449999888\n",
            "Corporate\t706146.3668000001\n",
            "Home Office\t429653.1485000003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hive -S -database bikramDB -e \"\\\n",
        "SELECT segment, region, category, round(SUM (sales),2) sales \\\n",
        "FROM ss_order2 \\\n",
        "GROUP BY segment, region, category; \\\n",
        "\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFDKHxADBWBo",
        "outputId": "b77396fe-85d6-40fa-de50-5341ef97c8cd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hive Session ID = df26f0fe-5b87-47c4-b01b-fa9f37470338\n",
            "Hive Session ID = cff561e9-87df-4e7b-a515-14bc84f103ba\n",
            "Consumer\tCentral\tFurniture\t86229.22\n",
            "Consumer\tCentral\tOffice Supplies\t93111.48\n",
            "Consumer\tCentral\tTechnology\t72690.74\n",
            "Consumer\tEast\tFurniture\t114211.8\n",
            "Consumer\tEast\tOffice Supplies\t101255.14\n",
            "Consumer\tEast\tTechnology\t135441.23\n",
            "Consumer\tSouth\tFurniture\t70800.2\n",
            "Consumer\tSouth\tOffice Supplies\t59504.58\n",
            "Consumer\tSouth\tTechnology\t65276.19\n",
            "Consumer\tWest\tFurniture\t119808.09\n",
            "Consumer\tWest\tOffice Supplies\t110080.94\n",
            "Consumer\tWest\tTechnology\t132991.75\n",
            "Corporate\tCentral\tFurniture\t52085.6\n",
            "Corporate\tCentral\tOffice Supplies\t41137.7\n",
            "Corporate\tCentral\tTechnology\t64772.51\n",
            "Corporate\tEast\tFurniture\t64209.05\n",
            "Corporate\tEast\tOffice Supplies\t66474.74\n",
            "Corporate\tEast\tTechnology\t69725.57\n",
            "Corporate\tSouth\tFurniture\t29645.03\n",
            "Corporate\tSouth\tOffice Supplies\t45930.17\n",
            "Corporate\tSouth\tTechnology\t46310.73\n",
            "Corporate\tWest\tFurniture\t83080.11\n",
            "Corporate\tWest\tOffice Supplies\t77133.86\n",
            "Corporate\tWest\tTechnology\t65641.31\n",
            "Home Office\tCentral\tFurniture\t25482.34\n",
            "Home Office\tCentral\tOffice Supplies\t32777.24\n",
            "Home Office\tCentral\tTechnology\t32953.07\n",
            "Home Office\tEast\tFurniture\t29870.36\n",
            "Home Office\tEast\tOffice Supplies\t37786.18\n",
            "Home Office\tEast\tTechnology\t59807.19\n",
            "Home Office\tSouth\tFurniture\t16853.45\n",
            "Home Office\tSouth\tOffice Supplies\t20216.56\n",
            "Home Office\tSouth\tTechnology\t37184.99\n",
            "Home Office\tWest\tFurniture\t49724.55\n",
            "Home Office\tWest\tOffice Supplies\t33638.45\n",
            "Home Office\tWest\tTechnology\t53358.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Serde\n",
        "https://stackoverflow.com/questions/13628658/hive-load-csv-with-commas-in-quoted-fields <br>\n",
        "https://cwiki.apache.org/confluence/display/Hive/CSV+Serde"
      ],
      "metadata": {
        "id": "6JUtxoJJgZ79"
      }
    }
  ]
}